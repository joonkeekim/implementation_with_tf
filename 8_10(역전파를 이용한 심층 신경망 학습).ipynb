{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_10(역전파를 이용한 심층 신경망 학습).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNUjDtQvFPLN0SrYjb2ielm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xccZ0nPcM30l","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597060165583,"user_tz":-540,"elapsed":1944,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["import time\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ISJo90ucNKky","colab_type":"text"},"source":["#유틸"]},{"cell_type":"code","metadata":{"id":"b0wRUgXONARo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597060191581,"user_tz":-540,"elapsed":623,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["def _t(x):\n","    return np.transpose(x)\n","def _m(A,B):\n","    return np.matmul(A,B)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"MNxqMItJNJLE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597065524398,"user_tz":-540,"elapsed":638,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["#전과 다르게 class로 하는 이유는 dp를 위한 메모리확보를 위해\n","\n","class Sigmoid:\n","    def __init__(self):\n","        self.last_o=1#마지막 출력을 저장하게 해줌\n","    def __call__(self,x):\n","        self.last_o = 1/(1.0+np.exp(-x))#미리저장\n","        return self.last_o\n","\n","    def grad(self):#미분하면 sig/(1-sig)\n","        return self.last_o*(1-self.last_o)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kqioLT-NW8f","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597061108228,"user_tz":-540,"elapsed":660,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["class MeanSquareError:\n","    def __init__(self):\n","        #grad\n","        self.dh = 1#마지막 출력을 저장하게 해줌\n","\n","        #for bp\n","        self.last_diff = 1\n","    def __call__(self,h,y):#mse는 1/2*mean(h-y)^2\n","        self.last_diff = h-y\n","        return 1/2*np.mean(np.square(h-y))\n","    def grad(self):#h-y\n","        return self.last_diff"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8lbHqx9NlCp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597068616040,"user_tz":-540,"elapsed":677,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["class Neuron:\n","    #말만 뉴런이지 layer한 층(벡터화 시켰으니까)\n","    def __init__(self,W,b,a_obj):\n","        self.W = W\n","        self.b = b\n","        self.a = a_obj()\n","\n","        #grad\n","        self.dW = np.zeros_like(self.W)\n","        self.db = np.zeros_like(self.b)\n","        #dh = Mx1\n","        self.dh = np.zeros_like(_t(self.W))\n","        #print(\"_t(self.W):{}\".format(self.dh.shape))\n","        self.last_x = np.zeros((self.W.shape[0]))#마지막x를 저장 W를 미분하면 h layer하나 나옴. 즉 for grad_W\n","        #print(\"last_x.shape:{}\".format(self.last_x.shape))\n","\n","\n","    def __call__(self,x):\n","        self.last_x = x\n","        self.last_h = _m(_t(self.W),x)+self.b\n","        return self.a(self.last_h)\n","\n","\n","    #y = w^Tx+b\n","    #각 layer의 입력과 출력사이의 gradient\n","    def grad(self):# dy/dh = W, h는 한 layer층의 벡터를 말함\n","        #print(\"W*a shape : {}\".format((self.W*self.a.grad()).shape))\n","        return self.W*self.a.grad()\n","\n","    def grad_W(self, dh):#dh는 누적되서 온 gradient\n","        #print(\"W dh shape : {}\".format(dh.shape))\n","        grad = np.ones_like(self.W)\n","        grad_a = self.a.grad()\n","        for j in range(grad.shape[1]):#출력 쪽 dimension , y = w^Tx+b에서 w로 편미분이라 x만 남음 dy/dw = x\n","            grad[:,j] = dh[j]*grad_a[j]*self.last_x#전의 layer부터 내려오는 값들을 저장, 그 뒤는 acti미분 * h (n-1)\n","        return grad\n","\n","    def grad_b(self, dh):# dy/dh = 1\n","        #print(\"b dh shape : {}\".format(dh.shape))\n","        return dh * self.a.grad()*1"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"BNDxbVArN-AD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597067604703,"user_tz":-540,"elapsed":518,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["class DNN:\n","    def __init__(self,hidden_depth, num_neuron, num_input, num_output, activation=Sigmoid):\n","        def init_var(i, o):#input과 output의 뉴런의 개수\n","            return np.random.normal(0.0, 0.01, (i,o)), np.zeros((o,))#뒤는 bias\n","\n","        self.sequence = list()\n","        #1st\n","        W, b = init_var(num_input,num_neuron)\n","        self.sequence.append(Neuron(W,b,activation))\n","\n","        #other hidden\n","        for _ in range(hidden_depth):\n","            W, b = init_var(num_neuron, num_neuron)\n","            self.sequence.append(Neuron(W,b,activation))\n","\n","        #output\n","        W, b = init_var(num_neuron,num_output)\n","        self.sequence.append(Neuron(W,b,activation))\n","\n","    def __call__(self,x):\n","        for layer in self.sequence:#시퀀스의 레이어를 호출해서 반복\n","            x = layer(x)\n","        return x\n","\n","    def calc_gradient(self,loss_obj):\n","        #gradient 구현\n","        #핵심!!\n","        #전과 다르게 forward inference한번으로 저장된 값을 통해한다.\n","        loss_obj.dh = loss_obj.grad()\n","        self.sequence.append(loss_obj)\n","        #이렇게하면 loss가 있어 call할 때 출력을 못 얻고 loss만 얻는다.\n","        #back propagation\n","        for i in range(len(self.sequence) - 1,0,-1):#이러면 역순으로 나온다.for문이랑 똑같음\n","            #하나씩 진행할거다.\n","            l1 = self.sequence[i]\n","            l0 = self.sequence[i-1]\n","            #l1는 전꺼\n","            l0.dh = _m(l0.grad(), l1.dh)#loss를 현재 layer로 미분한 grad계산된다.\n","            l0.dW = l0.grad_W(l1.dh)\n","            l0.db = l0.grad_b(l1.dh)\n","\n","        self.sequence.remove(loss_obj)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PYskK1fMOF0L","colab_type":"text"},"source":["#경사 하강\u001b"]},{"cell_type":"code","metadata":{"id":"c6EmpoXSOAEM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597067607728,"user_tz":-540,"elapsed":653,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}}},"source":["def gradient_descent(network,x,y,loss_obj, alpha = 0.01):\n","  loss = loss_obj(network(x),y)\n","  network.calc_gradient(loss_obj)\n","  for layer in network.sequence:\n","    layer.W += -alpha * layer.dW\n","    layer.b += -alpha * layer.db\n","  return loss"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJcoGoh9OJM3","colab_type":"text"},"source":["#실행\n","\n","##진짜 돌리자마자 되는정도의 차이!!!전엔 86초였다."]},{"cell_type":"code","metadata":{"id":"h0vsQ3z5OMfy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597068621345,"user_tz":-540,"elapsed":670,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"ce56bd13-c18c-4133-bf65-041e0e6f7f2e"},"source":["x = np.random.normal(0.0, 1.0, (10,))\n","y = np.random.normal(0.0, 1.0, (2,))\n","\n","t = time.time()\n","dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=Sigmoid)\n","loss_obj = MeanSquareError()\n","for epoch in range(100):\n","    loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01)\n","    print('Epoch {}: Test loss {}'.format(epoch, loss))\n","print('{} seconds elapsed.'.format(time.time() - t))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["last_x.shape:(10,)\n","last_x.shape:(32,)\n","last_x.shape:(32,)\n","last_x.shape:(32,)\n","last_x.shape:(32,)\n","last_x.shape:(32,)\n","last_x.shape:(32,)\n","Epoch 0: Test loss 0.7266304133699091\n","Epoch 1: Test loss 0.7184334380747983\n","Epoch 2: Test loss 0.7103327575727756\n","Epoch 3: Test loss 0.702332953114027\n","Epoch 4: Test loss 0.694438229400483\n","Epoch 5: Test loss 0.6866524108545641\n","Epoch 6: Test loss 0.6789789408978141\n","Epoch 7: Test loss 0.6714208840618043\n","Epoch 8: Test loss 0.663980930723616\n","Epoch 9: Test loss 0.6566614042348395\n","Epoch 10: Test loss 0.6494642701963045\n","Epoch 11: Test loss 0.6423911476204045\n","Epoch 12: Test loss 0.6354433217185315\n","Epoch 13: Test loss 0.6286217580521631\n","Epoch 14: Test loss 0.6219271177919747\n","Epoch 15: Test loss 0.6153597738392053\n","Epoch 16: Test loss 0.6089198275767154\n","Epoch 17: Test loss 0.6026071260329707\n","Epoch 18: Test loss 0.596421279259898\n","Epoch 19: Test loss 0.5903616777445224\n","Epoch 20: Test loss 0.5844275096939394\n","Epoch 21: Test loss 0.5786177780529794\n","Epoch 22: Test loss 0.572931317133434\n","Epoch 23: Test loss 0.5673668087526158\n","Epoch 24: Test loss 0.5619227977969616\n","Epoch 25: Test loss 0.5565977071432219\n","Epoch 26: Test loss 0.5513898518852826\n","Epoch 27: Test loss 0.5462974528288098\n","Epoch 28: Test loss 0.541318649228581\n","Epoch 29: Test loss 0.5364515107546235\n","Epoch 30: Test loss 0.531694048683097\n","Epoch 31: Test loss 0.527044226316332\n","Epoch 32: Test loss 0.5224999686435985\n","Epoch 33: Test loss 0.518059171260163\n","Epoch 34: Test loss 0.5137197085670551\n","Epoch 35: Test loss 0.5094794412778324\n","Epoch 36: Test loss 0.505336223261608\n","Epoch 37: Test loss 0.5012879077537691\n","Epoch 38: Test loss 0.4973323529673002\n","Epoch 39: Test loss 0.4934674271384914\n","Epoch 40: Test loss 0.48969101304117446\n","Epoch 41: Test loss 0.4860010120035514\n","Epoch 42: Test loss 0.48239534746124074\n","Epoch 43: Test loss 0.4788719680794375\n","Epoch 44: Test loss 0.47542885047611394\n","Epoch 45: Test loss 0.4720640015770381\n","Epoch 46: Test loss 0.4687754606320973\n","Epoch 47: Test loss 0.4655613009210277\n","Epoch 48: Test loss 0.46241963117519913\n","Epoch 49: Test loss 0.45934859674061745\n","Epoch 50: Test loss 0.4563463805058032\n","Epoch 51: Test loss 0.4534112036167146\n","Epoch 52: Test loss 0.4505413259994134\n","Epoch 53: Test loss 0.4477350467097331\n","Epoch 54: Test loss 0.44499070412782693\n","Epoch 55: Test loss 0.4423066760141327\n","Epoch 56: Test loss 0.43968137944201696\n","Epoch 57: Test loss 0.4371132706211484\n","Epoch 58: Test loss 0.43460084462449927\n","Epoch 59: Test loss 0.43214263503078887\n","Epoch 60: Test loss 0.42973721349317195\n","Epoch 61: Test loss 0.427383189244014\n","Epoch 62: Test loss 0.4250792085447132\n","Epoch 63: Test loss 0.42282395408869766\n","Epoch 64: Test loss 0.42061614436496353\n","Epoch 65: Test loss 0.4184545329888036\n","Epoch 66: Test loss 0.41633790800572557\n","Epoch 67: Test loss 0.4142650911739514\n","Epoch 68: Test loss 0.4122349372303338\n","Epoch 69: Test loss 0.410246333144015\n","Epoch 70: Test loss 0.40829819736168893\n","Epoch 71: Test loss 0.40638947904789613\n","Epoch 72: Test loss 0.4045191573233913\n","Epoch 73: Test loss 0.4026862405042728\n","Epoch 74: Test loss 0.40088976534423204\n","Epoch 75: Test loss 0.39912879628199394\n","Epoch 76: Test loss 0.3974024246957504\n","Epoch 77: Test loss 0.3957097681661459\n","Epoch 78: Test loss 0.3940499697491619\n","Epoch 79: Test loss 0.3924221972600441\n","Epoch 80: Test loss 0.39082564256924474\n","Epoch 81: Test loss 0.38925952091119\n","Epoch 82: Test loss 0.38772307020654234\n","Epoch 83: Test loss 0.3862155503984973\n","Epoch 84: Test loss 0.38473624280354457\n","Epoch 85: Test loss 0.3832844494770154\n","Epoch 86: Test loss 0.3818594925936544\n","Epoch 87: Test loss 0.38046071384337143\n","Epoch 88: Test loss 0.3790874738422542\n","Epoch 89: Test loss 0.3777391515588695\n","Epoch 90: Test loss 0.37641514375581564\n","Epoch 91: Test loss 0.3751148644464488\n","Epoch 92: Test loss 0.37383774436666045\n","Epoch 93: Test loss 0.3725832304615478\n","Epoch 94: Test loss 0.37135078538678856\n","Epoch 95: Test loss 0.37013988702450507\n","Epoch 96: Test loss 0.36895002801337906\n","Epoch 97: Test loss 0.36778071529276196\n","Epoch 98: Test loss 0.3666314696605073\n","Epoch 99: Test loss 0.36550182534424125\n","0.22121953964233398 seconds elapsed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FxT6NnxOtpfj","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}