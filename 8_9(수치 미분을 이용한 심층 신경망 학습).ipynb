{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_9(수치 미분을 이용한 심층 신경망 학습).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9Suqda1AV8NMTB4pxI5Ez"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"o9XZicJzTG96","colab_type":"code","colab":{}},"source":["import time\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5BA6pfNTQ-A","colab_type":"text"},"source":["#유틸\n"]},{"cell_type":"code","metadata":{"id":"I5qDocH0TTES","colab_type":"code","colab":{}},"source":["epsilon = 0.0001\n","\n","def _t(x):\n","  return np.transpose(x)\n","\n","def _m(A, B):\n","  return np.matmul(A,B)\n","\n","def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","def mean_squared_error(h, y):\n","  return 1/2*np.mean(np.square(h-y))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtHfoW98TsEl","colab_type":"text"},"source":["#뉴런 구현"]},{"cell_type":"code","metadata":{"id":"cetJvvlATtnA","colab_type":"code","colab":{}},"source":["class Neuron:\n","    def __init__(self, W, b, a):\n","    #parameter\n","        self.W = W\n","        self.b = b\n","        self.a = a\n","    #gradients를 저장하기 위해\n","        self.dW = np.zeros_like((self.W))\n","        self.db = np.zeros_like((self.b))\n","    def __call__(self, x):\n","        return self.a(_m(_t(self.W),x) + self.b) #W^Tx + b 니까, W가 아직 transpose전이니까\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EDOvOjTJUpoK","colab_type":"text"},"source":["#심층 신경망 구현"]},{"cell_type":"code","metadata":{"id":"dtPOAwHtUsJU","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7W1IIL66eCUg","colab_type":"code","colab":{}},"source":["class DNN:\n","  def __init__(self,hidden_depth, num_neuron, num_input, num_output, activation = sigmoid):\n","    def init_var(i, o):#input과 output의 뉴런의 개수\n","      return np.random.normal(0.0, 0.01, (i,o)), np.zeros((o,))#뒤는 bias\n","\n","    self.sequence = list()\n","    #1st\n","    W, b = init_var(num_input,num_neuron)\n","    self.sequence.append(Neuron(W,b,activation))\n","\n","    #other hidden\n","    for _ in range(hidden_depth - 1):\n","      W, b = init_var(num_neuron, num_neuron)\n","      self.sequence.append(Neuron(W,b,activation))\n","\n","    #output\n","    W, b = init_var(num_neuron,num_output)\n","    self.sequence.append(Neuron(W,b,activation))\n","\n","  def __call__(self,x):\n","    for layer in self.sequence:#시퀀스의 레이어를 호출해서 반복\n","      x = layer(x)\n","    return x\n","\n","  def calc_gradient(self,x,y,loss_func):#x는 입력, y는 정답\n","    #gradient 구현\n","    #핵심!!\n","    def get_new_sequence(layer_index, new_neuron):\n","      new_sequence = list()\n","      for i, layer in enumerate(self.sequence):\n","        if i == layer_index:\n","          new_sequence.append(new_neuron)\n","        else:\n","          new_sequence.append(layer)\n","      return new_sequence\n","      \n","    def eval_sequence(x, sequence):\n","      for layer in sequence:\n","        x = layer(x)\n","      return x\n","    \n","    loss = loss_func(self(x), y)#self는 dnn이라 self.__init__(x)임\n","    \n","    for layer_id, layer in enumerate(self.sequence):#iterate layer\n","      for w_i, w in enumerate(layer.W):#iterate W(row)\n","        for w_j, ww in enumerate(w):#iterate W(col)\n","          W = np.copy(layer.W)\n","          W[w_i][w_j] = ww+epsilon\n","\n","          new_seq = get_new_sequence(layer_id,Neuron(W,layer.b,layer.a))\n","          h = eval_sequence(x,new_seq)#하나만 변경한 상태에ㅔ서 평가\n","\n","          num_grad = (loss_func(h,y)-loss)/epsilon#new eval과 정답을 넣은 loss다시 계산\n","          #class neuron에 d뭐시기로 다 저장시킨다.\n","          layer.dW[w_i][w_j] = num_grad\n","          \n","        for b_i, bb in enumerate(layer.b):\n","          b = np.copy(layer.b)\n","          b[b_i] = bb +epsilon\n","\n","          new_seq = get_new_sequence(layer_id,Neuron(layer.W,b,layer.a))\n","          h = eval_sequence(x,new_seq)\n","\n","          num_grad = (loss_func(h,y)-loss)/epsilon\n","          layer.db[b_i] = num_grad\n","\n","    return loss#loss확인 시키기 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"029GXsAnOC_E","colab_type":"text"},"source":["#경사 하강"]},{"cell_type":"code","metadata":{"id":"H7TmqpmUWpeL","colab_type":"code","colab":{}},"source":["def gradient_descent(network,x,y,loss_obj, alpha = 0.01):\n","  loss = network.calc_gradient(x,y,loss_obj)\n","  for layer in network.sequence:\n","    layer.W += -alpha * layer.dW\n","    layer.b += -alpha * layer.db\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6QdgL-Jd8ZQ","colab_type":"text"},"source":["#실행"]},{"cell_type":"code","metadata":{"id":"PUx6OKvAc1lN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596984043588,"user_tz":-540,"elapsed":87644,"user":{"displayName":"김준기","photoUrl":"","userId":"08456149661061922976"}},"outputId":"d8373320-5fdf-43ef-9b21-386f9e9e3e02"},"source":["x = np.random.normal(0.0,1.0,(10,))\n","y = np.random.normal(0.0,1.0,(2,))\n","\n","dnn = DNN(hidden_depth=5,num_neuron=32,num_input=10,num_output=2,activation=sigmoid)\n","\n","t = time.time()\n","for epoch in range(100):\n","  loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)\n","  print('epoch {}, test loss {}'.format(epoch, loss))\n","print('{} seconds elapsed'.format(time.time()-t))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch 0, test loss 0.056849490790312186\n","epoch 1, test loss 0.056530615107248376\n","epoch 2, test loss 0.05621351606872431\n","epoch 3, test loss 0.055898190543064864\n","epoch 4, test loss 0.05558463524694437\n","epoch 5, test loss 0.05527284674826616\n","epoch 6, test loss 0.05496282146905161\n","epoch 7, test loss 0.054654555688315565\n","epoch 8, test loss 0.054348045544940916\n","epoch 9, test loss 0.05404328704055891\n","epoch 10, test loss 0.05374027604240778\n","epoch 11, test loss 0.05343900828620479\n","epoch 12, test loss 0.05313947937898594\n","epoch 13, test loss 0.05284168480195741\n","epoch 14, test loss 0.05254561991332504\n","epoch 15, test loss 0.052251279951113234\n","epoch 16, test loss 0.05195866003597652\n","epoch 17, test loss 0.05166775517398354\n","epoch 18, test loss 0.05137856025940098\n","epoch 19, test loss 0.05109107007745359\n","epoch 20, test loss 0.05080527930706754\n","epoch 21, test loss 0.05052118252359794\n","epoch 22, test loss 0.050238774201533554\n","epoch 23, test loss 0.04995804871718185\n","epoch 24, test loss 0.049679000351337765\n","epoch 25, test loss 0.049401623291926475\n","epoch 26, test loss 0.04912591163662455\n","epoch 27, test loss 0.048851859395461245\n","epoch 28, test loss 0.04857946049338571\n","epoch 29, test loss 0.048308708772826975\n","epoch 30, test loss 0.048039597996213865\n","epoch 31, test loss 0.047772121848472675\n","epoch 32, test loss 0.04750627393950522\n","epoch 33, test loss 0.04724204780663091\n","epoch 34, test loss 0.04697943691700666\n","epoch 35, test loss 0.04671843467002101\n","epoch 36, test loss 0.04645903439965059\n","epoch 37, test loss 0.04620122937680541\n","epoch 38, test loss 0.04594501281162491\n","epoch 39, test loss 0.045690377855756366\n","epoch 40, test loss 0.04543731760460453\n","epoch 41, test loss 0.045185825099548306\n","epoch 42, test loss 0.04493589333012475\n","epoch 43, test loss 0.04468751523618529\n","epoch 44, test loss 0.044440683710025325\n","epoch 45, test loss 0.044195391598476234\n","epoch 46, test loss 0.04395163170497241\n","epoch 47, test loss 0.04370939679158018\n","epoch 48, test loss 0.04346867958100252\n","epoch 49, test loss 0.043229472758549874\n","epoch 50, test loss 0.042991768974078796\n","epoch 51, test loss 0.0427555608439017\n","epoch 52, test loss 0.042520840952659796\n","epoch 53, test loss 0.04228760185517243\n","epoch 54, test loss 0.042055836078250274\n","epoch 55, test loss 0.04182553612247735\n","epoch 56, test loss 0.04159669446396211\n","epoch 57, test loss 0.041369303556057786\n","epoch 58, test loss 0.04114335583105069\n","epoch 59, test loss 0.04091884370182173\n","epoch 60, test loss 0.040695759563468284\n","epoch 61, test loss 0.04047409579490623\n","epoch 62, test loss 0.040253844760429126\n","epoch 63, test loss 0.040034998811249965\n","epoch 64, test loss 0.039817550286999986\n","epoch 65, test loss 0.03960149151721018\n","epoch 66, test loss 0.03938681482274851\n","epoch 67, test loss 0.03917351251723974\n","epoch 68, test loss 0.03896157690845002\n","epoch 69, test loss 0.038751000299643\n","epoch 70, test loss 0.038541774990907024\n","epoch 71, test loss 0.0383338932804516\n","epoch 72, test loss 0.03812734746587927\n","epoch 73, test loss 0.03792212984542713\n","epoch 74, test loss 0.03771823271917808\n","epoch 75, test loss 0.03751564839024669\n","epoch 76, test loss 0.037314369165938825\n","epoch 77, test loss 0.03711438735888181\n","epoch 78, test loss 0.03691569528812723\n","epoch 79, test loss 0.03671828528022998\n","epoch 80, test loss 0.036522149670296385\n","epoch 81, test loss 0.03632728080301154\n","epoch 82, test loss 0.03613367103363787\n","epoch 83, test loss 0.03594131272898842\n","epoch 84, test loss 0.03575019826837607\n","epoch 85, test loss 0.03556032004453736\n","epoch 86, test loss 0.035371670464529804\n","epoch 87, test loss 0.03518424195061271\n","epoch 88, test loss 0.034998026941093406\n","epoch 89, test loss 0.034813017891157566\n","epoch 90, test loss 0.03462920727367437\n","epoch 91, test loss 0.034446587579980446\n","epoch 92, test loss 0.034265151320634936\n","epoch 93, test loss 0.0340848910261627\n","epoch 94, test loss 0.03390579924776742\n","epoch 95, test loss 0.033727868558026335\n","epoch 96, test loss 0.03355109155156477\n","epoch 97, test loss 0.033375460845709\n","epoch 98, test loss 0.033200969081116874\n","epoch 99, test loss 0.0330276089223908\n","86.80322241783142 seconds elapsed\n"],"name":"stdout"}]}]}